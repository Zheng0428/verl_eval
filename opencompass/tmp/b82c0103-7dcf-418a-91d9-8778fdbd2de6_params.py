datasets = [
    [
        dict(
            abbr='lukaemon_mmlu_college_biology_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about college biology. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about college biology. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='college_biology',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[72:144]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_college_chemistry_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about college chemistry. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about college chemistry. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='college_chemistry',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_college_computer_science_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about college computer science. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about college computer science. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='college_computer_science',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_college_mathematics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about college mathematics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about college mathematics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='college_mathematics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_college_physics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about college physics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about college physics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='college_physics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[51:102]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_electrical_engineering_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about electrical engineering. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about electrical engineering. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='electrical_engineering',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[73:146]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_astronomy_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about astronomy. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about astronomy. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='astronomy',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[76:152]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_anatomy_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about anatomy. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about anatomy. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='anatomy',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[68:136]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_abstract_algebra_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about abstract algebra. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about abstract algebra. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='abstract_algebra',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_machine_learning_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about machine learning. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about machine learning. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='machine_learning',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[56:112]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_clinical_knowledge_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about clinical knowledge. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about clinical knowledge. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='clinical_knowledge',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[133:266]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_global_facts_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about global facts. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about global facts. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='global_facts',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_management_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about management. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about management. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='management',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[52:104]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_nutrition_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about nutrition. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about nutrition. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='nutrition',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[153:306]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_marketing_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about marketing. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about marketing. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='marketing',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[117:234]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_professional_accounting_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about professional accounting. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about professional accounting. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='professional_accounting',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[141:282]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_geography_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school geography. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school geography. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_geography',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[99:198]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_international_law_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about international law. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about international law. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='international_law',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[61:122]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_moral_scenarios_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about moral scenarios. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about moral scenarios. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='moral_scenarios',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[448:896]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_computer_security_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about computer security. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about computer security. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='computer_security',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_microeconomics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school microeconomics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school microeconomics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_microeconomics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[119:238]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_professional_law_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about professional law. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about professional law. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='professional_law',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[767:1534]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_medical_genetics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about medical genetics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about medical genetics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='medical_genetics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_professional_psychology_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about professional psychology. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about professional psychology. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='professional_psychology',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[306:612]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_jurisprudence_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about jurisprudence. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about jurisprudence. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='jurisprudence',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[54:108]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_world_religions_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about world religions. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about world religions. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='world_religions',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[86:172]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_philosophy_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about philosophy. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about philosophy. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='philosophy',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[156:312]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_virology_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about virology. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about virology. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='virology',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[83:166]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_chemistry_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school chemistry. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school chemistry. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_chemistry',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[102:204]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_public_relations_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about public relations. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about public relations. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='public_relations',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[55:110]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_macroeconomics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school macroeconomics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school macroeconomics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_macroeconomics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[195:390]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_human_sexuality_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about human sexuality. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about human sexuality. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='human_sexuality',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[66:132]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_elementary_mathematics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about elementary mathematics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about elementary mathematics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='elementary_mathematics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[189:378]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_physics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school physics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school physics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_physics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[76:152]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_computer_science_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school computer science. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school computer science. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_computer_science',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_european_history_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school european history. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school european history. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_european_history',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[83:166]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_business_ethics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about business ethics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about business ethics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='business_ethics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_moral_disputes_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about moral disputes. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about moral disputes. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='moral_disputes',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[173:346]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_statistics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school statistics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school statistics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_statistics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[108:216]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_miscellaneous_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about miscellaneous. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about miscellaneous. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='miscellaneous',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[392:784]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_formal_logic_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about formal logic. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about formal logic. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='formal_logic',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[63:126]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_government_and_politics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school government and politics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school government and politics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_government_and_politics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[97:194]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_prehistory_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about prehistory. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about prehistory. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='prehistory',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[162:324]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_security_studies_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about security studies. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about security studies. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='security_studies',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[123:246]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_biology_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school biology. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school biology. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_biology',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[155:310]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_logical_fallacies_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about logical fallacies. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about logical fallacies. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='logical_fallacies',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[82:164]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_world_history_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school world history. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school world history. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_world_history',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[119:238]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_professional_medicine_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about professional medicine. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about professional medicine. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='professional_medicine',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[136:272]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_mathematics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school mathematics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school mathematics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_mathematics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[135:270]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_college_medicine_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about college medicine. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about college medicine. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='college_medicine',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[87:174]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_us_history_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school us history. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school us history. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_us_history',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[102:204]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_sociology_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about sociology. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about sociology. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='sociology',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[101:202]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_econometrics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about econometrics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about econometrics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='econometrics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[57:114]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_high_school_psychology_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school psychology. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about high school psychology. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='high_school_psychology',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[273:546]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_human_aging_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about human aging. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about human aging. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='human_aging',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[112:224]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_us_foreign_policy_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about us foreign policy. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about us foreign policy. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='us_foreign_policy',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[50:100]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
        dict(
            abbr='lukaemon_mmlu_conceptual_physics_1',
            eval_cfg=dict(
                evaluator=dict(
                    type='opencompass.openicl.icl_evaluator.AccEvaluator'),
                pred_postprocessor=dict(
                    options='ABCD',
                    type=
                    'opencompass.utils.text_postprocessors.first_option_postprocess'
                )),
            infer_cfg=dict(
                ice_template=dict(
                    template=dict(round=[
                        dict(
                            prompt=
                            'There is a single choice question about conceptual physics. Answer the question by replying A, B, C or D.\nQ: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nA: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n', role='BOT'),
                    ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                inferencer=dict(
                    max_out_len=8192,
                    type='opencompass.openicl.icl_inferencer.GenInferencer'),
                prompt_template=dict(
                    ice_token='</E>',
                    template=dict(
                        begin='</E>',
                        round=[
                            dict(
                                prompt=
                                "Question:\nThere is a single choice question about conceptual physics. Answer the question by replying A, B, C or D. {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:\nLet's think step by step.\n",
                                role='HUMAN'),
                        ]),
                    type=
                    'opencompass.openicl.icl_prompt_template.PromptTemplate'),
                retriever=dict(
                    type='opencompass.openicl.icl_retriever.ZeroRetriever')),
            name='conceptual_physics',
            path='opencompass/mmlu',
            reader_cfg=dict(
                input_columns=[
                    'input',
                    'A',
                    'B',
                    'C',
                    'D',
                ],
                output_column='target',
                test_range='[118:236]',
                train_split='dev'),
            type='opencompass.datasets.MMLUDataset'),
    ],
]
models = [
    dict(
        abbr='huggingface_hf-vllm',
        batch_size=8,
        generation_kwargs=dict(stop_token_ids=None),
        max_out_len=8192,
        max_seq_len=None,
        model_kwargs=dict(max_model_len=None, tensor_parallel_size=1),
        path=
        '/cfs2/hadoop-aipnlp/zengweihao02/checkpoints/verl-grpo-mistral-7b-rollout-1024-256mini-remove-reward-tem1.0-fixv1-fix_abel_remove_gsm8k_level1_Mistral-7B-v0.1/global_step_100/actor/huggingface',
        run_cfg=dict(num_gpus=1),
        type='opencompass.models.vllm.VLLM'),
]
work_dir = '/cfs2/hadoop-aipnlp/zengweihao02/checkpoints/verl-grpo-mistral-7b-rollout-1024-256mini-remove-reward-tem1.0-fixv1-fix_abel_remove_gsm8k_level1_Mistral-7B-v0.1/global_step_100/actor/huggingface/mmlu_abel/qwen_base/20250317_171452'
